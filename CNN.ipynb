{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CROP IMAGE TO REDUCE NOISE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved photo_10_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_10_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_10_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_11_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_11_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_12_2024-05-05_23-57-32.jpg\n",
      "Processed and saved photo_12_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_12_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_13_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_15_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_19_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_1_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_20_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_21_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_22_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_23_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_27_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_2_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_2_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_2_2024-05-06_00-51-20.jpg\n",
      "Processed and saved photo_3_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_3_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_3_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_4_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_4_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_4_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_4_2024-05-06_00-51-20.jpg\n",
      "Processed and saved photo_4_2024-05-06_01-07-21.jpg\n",
      "Processed and saved photo_5_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_5_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_5_2024-05-06_00-51-20.jpg\n",
      "Processed and saved photo_6_2024-05-05_23-57-32.jpg\n",
      "Processed and saved photo_6_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_6_2024-05-06_00-51-20.jpg\n",
      "Processed and saved photo_7_2024-05-05_23-57-32.jpg\n",
      "Processed and saved photo_7_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_8_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_8_2024-05-05_23-57-32.jpg\n",
      "Processed and saved photo_8_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_8_2024-05-06_00-46-23.jpg\n",
      "Processed and saved photo_9_2024-05-05_23-20-13.jpg\n",
      "Processed and saved photo_9_2024-05-05_23-57-32.jpg\n",
      "Processed and saved photo_9_2024-05-06_00-01-10.jpg\n",
      "Processed and saved photo_9_2024-05-06_00-46-23.jpg\n",
      "Total cropped images saved: 44\n",
      "Processed and saved photo_1_2024-05-05_23-56-55.jpg\n",
      "Processed and saved photo_1_2024-05-05_23-59-11.jpg\n",
      "Processed and saved photo_1_2024-05-06_00-43-35.jpg\n",
      "Processed and saved photo_1_2024-05-06_00-44-21.jpg\n",
      "Processed and saved photo_1_2024-05-06_01-43-22.jpg\n",
      "Processed and saved photo_1_2024-05-06_22-42-25.jpg\n",
      "Processed and saved photo_2_2024-05-05_23-18-33.jpg\n",
      "Processed and saved photo_2_2024-05-05_23-56-55.jpg\n",
      "Processed and saved photo_2_2024-05-06_00-43-35.jpg\n",
      "Processed and saved photo_2_2024-05-06_00-44-21.jpg\n",
      "Processed and saved photo_2_2024-05-06_01-43-22.jpg\n",
      "Processed and saved photo_3_2024-05-05_23-18-33.jpg\n",
      "Processed and saved photo_3_2024-05-05_23-56-55.jpg\n",
      "Processed and saved photo_3_2024-05-06_00-43-35.jpg\n",
      "Processed and saved photo_3_2024-05-06_00-44-21.jpg\n",
      "Processed and saved photo_3_2024-05-06_01-43-22.jpg\n",
      "Processed and saved photo_4_2024-05-05_23-18-33.jpg\n",
      "Processed and saved photo_4_2024-05-05_23-59-11.jpg\n",
      "Processed and saved photo_4_2024-05-06_00-43-35.jpg\n",
      "Processed and saved photo_4_2024-05-06_00-44-21.jpg\n",
      "Processed and saved photo_4_2024-05-06_01-43-22.jpg\n",
      "Processed and saved photo_4_2024-05-06_22-42-25.jpg\n",
      "Processed and saved photo_5_2024-05-05_23-18-33.jpg\n",
      "Processed and saved photo_5_2024-05-05_23-59-11.jpg\n",
      "Processed and saved photo_5_2024-05-06_01-43-22.jpg\n",
      "Processed and saved photo_6_2024-05-05_23-18-33.jpg\n",
      "Processed and saved photo_6_2024-05-06_22-42-25.jpg\n",
      "Processed and saved photo_7_2024-05-05_23-18-33.jpg\n",
      "Total cropped images saved: 28\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Function to preprocess images in a folder\n",
    "# Function to preprocess images in a folder and save only the cropped images\n",
    "def preprocess_images_in_folder(input_folder, output_folder, canny_threshold1=100, canny_threshold2=200):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Get a list of all files in the input folder\n",
    "    files = os.listdir(input_folder)\n",
    "    \n",
    "    # Counter for cropped images\n",
    "    cropped_image_count = 0\n",
    "    \n",
    "    # Iterate through each file in the input folder\n",
    "    for file in files:\n",
    "        # Check if the file is an image file (assuming all files in the folder are images)\n",
    "        if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "            # Read the image\n",
    "            image_path = os.path.join(input_folder, file)\n",
    "            image = cv2.imread(image_path)\n",
    "            \n",
    "            # Preprocess the image\n",
    "            _, cropped_object = preprocess_frame(image, canny_threshold1, canny_threshold2)\n",
    "            \n",
    "            # Check if the cropped object is not empty and its size is above 400x400\n",
    "            if cropped_object.size > 0 and cropped_object.shape[0] >= 340 and cropped_object.shape[1] >= 340 and cropped_object.shape[0] <= 700:\n",
    "                # Save the cropped object\n",
    "                cropped_image_count += 1\n",
    "                cropped_object_path = os.path.join(output_folder, f\"cropped_image_{cropped_image_count}.jpg\")\n",
    "                cv2.imwrite(cropped_object_path, cropped_object)\n",
    "                print(f\"Processed and saved {file}\")\n",
    "\n",
    "    print(f\"Total cropped images saved: {cropped_image_count}\")\n",
    "\n",
    "\n",
    "# Function to preprocess a single frame\n",
    "def preprocess_frame(frame, canny_threshold1, canny_threshold2, margin=10):\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform Canny edge detection with the specified thresholds\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "    # Find contours of the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if contours are found\n",
    "    if contours:\n",
    "        # Get the largest contour (assuming it's the object)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get the bounding box of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Add margin to the bounding box\n",
    "        x_margin = max(x - margin, 0)\n",
    "        y_margin = max(y - margin, 0)\n",
    "        w_margin = min(w + 2 * margin, frame.shape[1] - x_margin)\n",
    "        h_margin = min(h + 2 * margin, frame.shape[0] - y_margin)\n",
    "        \n",
    "        # Crop the object region with margin\n",
    "        cropped_object = frame[y_margin:y_margin+h_margin, x_margin:x_margin+w_margin]\n",
    "        \n",
    "        return frame, cropped_object\n",
    "    else:\n",
    "        # If no contours are found, return the original frame and an empty image\n",
    "        return frame, frame\n",
    "\n",
    "# Path to the input folder containing images\n",
    "input_folder = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/Unprocessed_Defect_Photo\"\n",
    "defect = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/defect\" \n",
    "input_folder1 = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/Unprocessed_Undefect_Photo\"\n",
    "undefect = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/undefect\" \n",
    "\n",
    "\n",
    "# Call the function to preprocess images in the input folder and save only the cropped images\n",
    "preprocess_images_in_folder(input_folder, defect)\n",
    "preprocess_images_in_folder(input_folder1, undefect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CANNY EDGE FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_1.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_2.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_3.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_4.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_5.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_6.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_7.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_8.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_9.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_10.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_11.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_12.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_13.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_14.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_15.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_16.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_17.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_18.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_19.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_20.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_21.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_22.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_23.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_24.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_25.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_26.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_27.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_28.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_29.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_30.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_31.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_32.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_33.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_34.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_35.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_36.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_37.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_38.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_39.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_40.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_41.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_42.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_43.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\\processed_image_44.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_1.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_2.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_3.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_4.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_5.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_6.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_7.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_8.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_9.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_10.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_11.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_12.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_13.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_14.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_15.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_16.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_17.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_18.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_19.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_20.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_21.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_22.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_23.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_24.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_25.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_26.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_27.jpg\n",
      "Processed image saved at: C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\\processed_image_28.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def preprocess_images(input_folder, output_folder):\n",
    "    # Ensure the output folder exists, create if not\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Process each image in the input folder\n",
    "    for i, filename in enumerate(os.listdir(input_folder)):\n",
    "        input_image_path = os.path.join(input_folder, filename)\n",
    "        output_image_path = os.path.join(output_folder, f\"processed_image_{i+1}.jpg\")\n",
    "\n",
    "        # Read the image\n",
    "        image = cv2.imread(input_image_path)\n",
    "\n",
    "        # Convert the image to grayscale\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(gray, threshold1=150, threshold2=50, apertureSize=3,L2gradient=True)  # Adjust thresholds as needed\n",
    "\n",
    "        # Save the processed image\n",
    "        cv2.imwrite(output_image_path, edges)\n",
    "        print(f\"Processed image saved at: {output_image_path}\")\n",
    "\n",
    "# Example usage\n",
    "input_folder = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/defect\"\n",
    "defect = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/defect\" \n",
    "input_folder1 = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/undefect\"\n",
    "undefect = r\"C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos/undefect\" \n",
    "preprocess_images(input_folder, defect)\n",
    "preprocess_images(input_folder1, undefect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 51 images belonging to 2 classes.\n",
      "Found 21 images belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">98</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">49</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">47</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,638,528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m98\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m49\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m47\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m1,638,528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,731,329</span> (6.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,731,329\u001b[0m (6.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,731,329</span> (6.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,731,329\u001b[0m (6.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.5294 - loss: 0.6932 - val_accuracy: 0.6190 - val_loss: 0.6419\n",
      "Epoch 2/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 85\u001b[0m\n\u001b[0;32m     82\u001b[0m my_callback \u001b[38;5;241m=\u001b[39m MyCallback()\n\u001b[0;32m     84\u001b[0m \u001b[38;5;66;03m# Train the model with the custom callback\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m          \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     88\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmy_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Save the trained model with the .h5 extension\u001b[39;00m\n\u001b[0;32m     91\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefect_detection_model_improved_4.keras\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Define input shape\n",
    "width, height = 100, 100  # Define your desired fixed size\n",
    "input_shape = (width, height, 1)  # Channels should be 1 since images are grayscale\n",
    "\n",
    "# Define data generators for training with data augmentation\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=360,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.3  # Define the validation split\n",
    ")\n",
    "\n",
    "# Define the directory containing your images\n",
    "data_dir = r'C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/photos'\n",
    "\n",
    "# Create train and validation generators\n",
    "batch_size = 100\n",
    "train_generator = train_data_gen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(width, height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='grayscale',  # Load images in grayscale\n",
    "    shuffle=True,\n",
    "    subset='training'  # Specify subset as 'training' for the training data\n",
    ")\n",
    "\n",
    "validation_generator = train_data_gen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(width, height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    color_mode='grayscale',  # Load images in grayscale\n",
    "    shuffle=True,\n",
    "    subset='validation'  # Specify subset as 'validation' for the validation data\n",
    ")\n",
    "\n",
    "# Define the CNN model with increased complexity\n",
    "def create_model(input_shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Conv2D(128, (3, 3), activation='relu'))  # Increase number of filters\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))  # Increase number of neurons\n",
    "    model.add(layers.Dropout(0.5))  # Add dropout for regularization\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification: defect or no defect\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = create_model(input_shape)\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Debugging: Print model summary for inspection\n",
    "print(model.summary())\n",
    "\n",
    "# Define a custom callback to stop training when validation loss reaches below a certain threshold\n",
    "class MyCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['val_loss'] < 0.55 and logs['accuracy']>0.8:\n",
    "            print(\"\\nReached validation loss below 0.4, stopping training!\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "my_callback = MyCallback()\n",
    "\n",
    "# Train the model with the custom callback\n",
    "model.fit(train_generator,\n",
    "          epochs=500,\n",
    "          validation_data=validation_generator,\n",
    "          callbacks=[my_callback])\n",
    "\n",
    "# Save the trained model with the .h5 extension\n",
    "model.save('defect_detection_model_improved_4.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Function to preprocess frames from the camera\n",
    "def preprocess_frame(frame, canny_threshold1, canny_threshold2):\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform Canny edge detection with the specified thresholds\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "    # Find contours of the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if contours are found\n",
    "    if contours:\n",
    "        # Get the largest contour (assuming it's the object)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get the bounding box of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Draw bounding box on the original frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Crop the object region\n",
    "        cropped_object = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        return frame, cropped_object\n",
    "    else:\n",
    "        # If no contours are found, return the original frame and an empty image\n",
    "        return frame, np.zeros_like(frame)\n",
    "\n",
    "# Function to update parameters based on trackbar positions\n",
    "def update_params_canny_threshold1(val):\n",
    "    global canny_threshold1\n",
    "    canny_threshold1 = val\n",
    "\n",
    "def update_params_canny_threshold2(val):\n",
    "    global canny_threshold2\n",
    "    canny_threshold2 = val\n",
    "\n",
    "# Initialize default parameters\n",
    "canny_threshold1 = 100\n",
    "canny_threshold2 = 200\n",
    "\n",
    "# Create window for trackbars\n",
    "cv2.namedWindow('Parameters')\n",
    "cv2.createTrackbar('Canny Threshold 1', 'Parameters', canny_threshold1, 500, update_params_canny_threshold1)\n",
    "cv2.createTrackbar('Canny Threshold 2', 'Parameters', canny_threshold2, 500, update_params_canny_threshold2)\n",
    "\n",
    "# Open camera\n",
    "cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Preprocess the frame\n",
    "    processed_frame, cropped_object = preprocess_frame(frame, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "    # Resize processed frame and cropped object to match the height of the original frame\n",
    "    target_height = frame.shape[0]\n",
    "    processed_frame_resized = cv2.resize(processed_frame, (int(target_height * processed_frame.shape[1] / processed_frame.shape[0]), target_height))\n",
    "    cropped_object_resized = cv2.resize(cropped_object, (int(target_height * cropped_object.shape[1] / cropped_object.shape[0]), target_height))\n",
    "    \n",
    "    # Concatenate frames horizontally\n",
    "    combined_frame = np.hstack((frame, processed_frame_resized, cropped_object_resized))\n",
    "\n",
    "    # Display the combined frame\n",
    "    cv2.imshow('Camera Preview | Processed Frame | Cropped Object', combined_frame)\n",
    "\n",
    "    # Break loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING THE BEST PARAMS FOR CONTOUR FINDING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pixel\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\tkinter\\__init__.py\", line 1948, in __call__\n",
      "    return self.func(*args)\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pixel\\AppData\\Local\\Temp\\ipykernel_24348\\1977063607.py\", line 63, in capture_image\n",
      "    cv2.imshow(\"Press 's' to capture\", frame)\n",
      "cv2.error: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'cv::imshow'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the trained model with .keras extension\n",
    "model = load_model('C:/Users/pixel/PycharmProjects/pythonProject/Arsitektur IoT/Computer Vision/defect_detection_model_improved.keras')\n",
    "\n",
    "# Define input shape\n",
    "width, height = 100, 100  # Same as the input shape used for training\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to the required input shape\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Perform Canny edge detection\n",
    "    edges = cv2.Canny(gray, threshold1=150, threshold2=50, apertureSize=3, L2gradient=True)  # You can adjust the parameters according to your preference\n",
    "    # Normalize the pixel values\n",
    "    edges = edges / 255.0\n",
    "    # Expand dimensions to match the input shape of the model\n",
    "    edges = np.expand_dims(edges, axis=-1)\n",
    "    # Expand dimensions to match the batch size\n",
    "    edges = np.expand_dims(edges, axis=0)\n",
    "    return edges\n",
    "\n",
    "def preprocess_frame(frame, canny_threshold1, canny_threshold2):\n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform Canny edge detection with the specified thresholds\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "    # Find contours of the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if contours are found\n",
    "    if contours:\n",
    "        # Get the largest contour (assuming it's the object)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get the bounding box of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Draw bounding box on the original frame\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "        # Crop the object region\n",
    "        cropped_object = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        return frame, cropped_object\n",
    "    else:\n",
    "        # If no contours are found, return the original frame and an empty image\n",
    "        return frame, np.zeros_like(frame)\n",
    "\n",
    "def capture_image():\n",
    "    # Access the camera\n",
    "    cap = cv2.VideoCapture(2, cv2.CAP_DSHOW)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"Press 's' to capture\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "            captured_frame = frame\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Preprocess the captured image with Canny edge filter\n",
    "    processed_image = preprocess_image(captured_frame)\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = model.predict(processed_image)\n",
    "\n",
    "    # Display the result\n",
    "    result_label.config(text=f\"Defect: {'Yes' if prediction[0][0] > 0.5 else 'No'}\")\n",
    "    accuracy_label.config(text=f\"Confidence: {round(prediction[0][0] * 100, 2)}%\")\n",
    "\n",
    "    # Preprocess the captured frame to find and crop objects\n",
    "    processed_frame, cropped_object = preprocess_frame(captured_frame, 100, 200)\n",
    "    \n",
    "    # Convert frame from OpenCV format to PIL format\n",
    "    img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Display the captured frame with processed frame and cropped object\n",
    "    frame_label.img = img  # To prevent garbage collection\n",
    "    frame_label.config(image=img)\n",
    "\n",
    "# Create GUI\n",
    "root = Tk()\n",
    "root.title(\"Defect Detection\")\n",
    "\n",
    "capture_button = Button(root, text=\"Capture Image\", command=capture_image)\n",
    "capture_button.pack(pady=10)\n",
    "\n",
    "result_label = Label(root, text=\"\")\n",
    "result_label.pack(pady=5)\n",
    "\n",
    "accuracy_label = Label(root, text=\"\")\n",
    "accuracy_label.pack(pady=5)\n",
    "\n",
    "# Label to display the captured frame\n",
    "frame_label = Label(root)\n",
    "frame_label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GUI FOR CHECKING DEFECT/NOT DEFECT OBJECT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 33 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001D50A38BBA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the trained model with .keras extension\n",
    "model = load_model('defect_detection_model_improved.keras')\n",
    "\n",
    "# Define input shape\n",
    "width, height = 100, 100  # Same as the input shape used for training\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to the required input shape\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Perform Canny edge detection\n",
    "    edges = cv2.Canny(gray, threshold1=150, threshold2=50, apertureSize=3, L2gradient=True)  # You can adjust the parameters according to your preference\n",
    "    # Normalize the pixel values\n",
    "    edges = edges / 255.0\n",
    "    # Expand dimensions to match the input shape of the model\n",
    "    edges = np.expand_dims(edges, axis=0)  # Remove the extra axis\n",
    "    return edges\n",
    "\n",
    "\n",
    "# def preprocess_frame(frame, canny_threshold1, canny_threshold2):\n",
    "#     # Convert frame to grayscale\n",
    "#     gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Perform Canny edge detection with the specified thresholds\n",
    "#     edges = cv2.Canny(gray, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "#     # Find contours of the edges\n",
    "#     contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "#     # Check if contours are found\n",
    "#     if contours:\n",
    "#         # Get the largest contour (assuming it's the object)\n",
    "#         largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "#         # Get the bounding box of the largest contour\n",
    "#         x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "#         # Draw bounding box on the original frame\n",
    "#         # cv2.rectangle(frame1, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        \n",
    "#         # Crop the object region\n",
    "#         cropped_object = frame[y:y+h, x:x+w]\n",
    "        \n",
    "#         return frame, cropped_object\n",
    "#     else:\n",
    "#         # If no contours are found, return the original frame and an empty image\n",
    "#         return frame, np.zeros_like(frame)\n",
    "\n",
    "def preprocess_frame(frame, canny_threshold1, canny_threshold2):\n",
    "    # Normalize the rotation to 0, 90, 180, or 270 degrees\n",
    "    rows, cols, _ = frame.shape\n",
    "    if rows > cols:\n",
    "        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        \n",
    "    # Normalize the pixel values and convert to 8-bit depth\n",
    "    normalized_frame = cv2.convertScaleAbs(frame / 255.0 * 255)\n",
    "    \n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(normalized_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform Canny edge detection with the specified thresholds\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "    # Find contours of the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if contours are found\n",
    "    if contours:\n",
    "        # Get the largest contour (assuming it's the object)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get the bounding box of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Crop the object region\n",
    "        cropped_object = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        return normalized_frame, cropped_object\n",
    "    else:\n",
    "        # If no contours are found, return the original frame and an empty image\n",
    "        return normalized_frame, np.zeros_like(frame)\n",
    "\n",
    "\n",
    "def capture_image():\n",
    "    # Access the camera\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"Press 's' to capture\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "            captured_frame = frame\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Preprocess the captured frame to find and crop objects\n",
    "    processed_frame, cropped_object = preprocess_frame(captured_frame, 100, 200)\n",
    "\n",
    "    # Preprocess the cropped object with Canny edge filter\n",
    "    processed_cropped_object = preprocess_image(cropped_object)\n",
    "\n",
    "    # Display the preprocessed cropped object\n",
    "    processed_cropped_img = Image.fromarray((processed_cropped_object[0] * 255).astype(np.uint8))\n",
    "    processed_cropped_img = ImageTk.PhotoImage(processed_cropped_img)\n",
    "    processed_label.img = processed_cropped_img\n",
    "    processed_label.config(image=processed_cropped_img)\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = model.predict(processed_cropped_object)\n",
    "    # Determine the threshold for defect presence   \n",
    "    threshold = 0.5\n",
    "\n",
    "    # Reverse the interpretation of the model's output\n",
    "    defect_presence = prediction[0][0] < threshold\n",
    "\n",
    "    # Update labels accordingly\n",
    "    result_label.config(text=f\"Defect: {'Yes' if defect_presence else 'No'}\")\n",
    "    if prediction > threshold:\n",
    "        accuracy_label.config(text=f\"Confidence: {round((prediction[0][0])* 100, 2)}%\")\n",
    "    else:\n",
    "        accuracy_label.config(text=f\"Confidence: {round((1 - prediction[0][0]) * 100, 2)}%\")\n",
    "\n",
    "    # Apply Canny edge detection to the cropped object\n",
    "    edges = cv2.Canny(cropped_object, threshold1=150, threshold2=50, apertureSize=3, L2gradient=True)\n",
    "\n",
    "    # Display the cropped object with Canny edges\n",
    "    cropped_img = Image.fromarray(edges)\n",
    "    cropped_img = ImageTk.PhotoImage(image=cropped_img)\n",
    "    cropped_label.img = cropped_img  # To prevent garbage collection\n",
    "    cropped_label.config(image=cropped_img)\n",
    "\n",
    "    # Convert frame from OpenCV format to PIL format\n",
    "    img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Display the captured frame with processed frame and cropped object\n",
    "    frame_label.img = img  # To prevent garbage collection\n",
    "    frame_label.config(image=img)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create GUI\n",
    "root = Tk()\n",
    "root.title(\"Defect Detection\")\n",
    "\n",
    "# Frame for the original frame and processed frame with bounding box\n",
    "frame_container = Frame(root)\n",
    "frame_container.pack(side=LEFT, padx=10, pady=10)\n",
    "\n",
    "# Label to display the captured frame with bounding box\n",
    "frame_label = Label(frame_container)\n",
    "frame_label.pack()\n",
    "\n",
    "# Label to display the cropped object with Canny edges\n",
    "cropped_label = Label(root)\n",
    "cropped_label.pack(side=RIGHT)\n",
    "\n",
    "# Label to display the processed image\n",
    "processed_label = Label(root)\n",
    "processed_label.pack()\n",
    "\n",
    "# Button to capture image\n",
    "capture_button = Button(root, text=\"Capture Image\", command=capture_image)\n",
    "capture_button.pack(pady=10)\n",
    "\n",
    "# Label to display the prediction result\n",
    "result_label = Label(root, text=\"\")\n",
    "result_label.pack(pady=5)\n",
    "\n",
    "# Label to display the prediction confidence\n",
    "accuracy_label = Label(root, text=\"\")\n",
    "accuracy_label.pack(pady=5)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tkinter import *\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "# Load the trained model with .keras extension\n",
    "model = load_model('defect_detection_model_improved_4.keras')\n",
    "\n",
    "# Define input shape\n",
    "width, height = 100, 100  # Same as the input shape used for training\n",
    "\n",
    "def preprocess_image(image):\n",
    "    # Resize the image to the required input shape\n",
    "    image = cv2.resize(image, (width, height))\n",
    "    # Convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # Perform Canny edge detection\n",
    "    edges = cv2.Canny(gray, threshold1=150, threshold2=50, apertureSize=3, L2gradient=True)  # You can adjust the parameters according to your preference\n",
    "    # Normalize the pixel values\n",
    "    edges = edges / 255.0\n",
    "    # Expand dimensions to match the input shape of the model\n",
    "    edges = np.expand_dims(edges, axis=0)  # Remove the extra axis\n",
    "    return edges\n",
    "\n",
    "def preprocess_frame(frame, canny_threshold1, canny_threshold2, margin=10):\n",
    "    # Normalize the rotation to 0, 90, 180, or 270 degrees\n",
    "    rows, cols, _ = frame.shape\n",
    "    if rows > cols:\n",
    "        frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)\n",
    "        \n",
    "    # Normalize the pixel values and convert to 8-bit depth\n",
    "    normalized_frame = cv2.convertScaleAbs(frame / 255.0 * 255)\n",
    "    \n",
    "    # Convert frame to grayscale\n",
    "    gray = cv2.cvtColor(normalized_frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Perform Canny edge detection with the specified thresholds\n",
    "    edges = cv2.Canny(gray, canny_threshold1, canny_threshold2)\n",
    "    \n",
    "    # Find contours of the edges\n",
    "    contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    \n",
    "    # Check if contours are found\n",
    "    if contours:\n",
    "        # Get the largest contour (assuming it's the object)\n",
    "        largest_contour = max(contours, key=cv2.contourArea)\n",
    "        \n",
    "        # Get the bounding box of the largest contour\n",
    "        x, y, w, h = cv2.boundingRect(largest_contour)\n",
    "        \n",
    "        # Add margins to the bounding box\n",
    "        x_margin = max(x - margin, 0)\n",
    "        y_margin = max(y - margin, 0)\n",
    "        w_margin = min(w + 2 * margin, frame.shape[1] - x_margin)\n",
    "        h_margin = min(h + 2 * margin, frame.shape[0] - y_margin)\n",
    "        \n",
    "        # Draw bounding box on the original frame\n",
    "        cv2.rectangle(normalized_frame, (x_margin, y_margin), (x_margin+w_margin, y_margin+h_margin), (0, 255, 0), 2)\n",
    "        \n",
    "        # Crop the object region\n",
    "        cropped_object = frame[y_margin:y_margin+h_margin, x_margin:x_margin+w_margin]\n",
    "        \n",
    "        return normalized_frame, cropped_object\n",
    "    else:\n",
    "        # If no contours are found, return the original frame and an empty image\n",
    "        return normalized_frame, np.zeros_like(frame)\n",
    "\n",
    "\n",
    "\n",
    "def capture_image():\n",
    "    # Access the camera\n",
    "    cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        cv2.imshow(\"Press 's' to capture\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('s'):\n",
    "            captured_frame = frame\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Preprocess the captured frame to find and crop objects\n",
    "    processed_frame, cropped_object = preprocess_frame(captured_frame, 100, 200)\n",
    "\n",
    "    # Preprocess the cropped object with Canny edge filter\n",
    "    processed_cropped_object = preprocess_image(cropped_object)\n",
    "\n",
    "    # Display the preprocessed cropped object\n",
    "    processed_cropped_img = Image.fromarray((processed_cropped_object[0] * 255).astype(np.uint8))\n",
    "    processed_cropped_img = ImageTk.PhotoImage(processed_cropped_img)\n",
    "    processed_label.img = processed_cropped_img\n",
    "    processed_label.config(image=processed_cropped_img)\n",
    "\n",
    "    # Predict using the model\n",
    "    prediction = model.predict(processed_cropped_object)\n",
    "    # Determine the threshold for defect presence   \n",
    "    threshold = 0.5\n",
    "\n",
    "    # Reverse the interpretation of the model's output\n",
    "    defect_presence = prediction[0][0] < threshold\n",
    "\n",
    "    # Update labels accordingly\n",
    "    result_label.config(text=f\"Defect: {'Yes' if defect_presence else 'No'}\")\n",
    "    if prediction > threshold:\n",
    "        accuracy_label.config(text=f\"Confidence: {round((prediction[0][0])* 100, 2)}%\")\n",
    "    else:\n",
    "        accuracy_label.config(text=f\"Confidence: {round((1-prediction[0][0]) * 100, 2)}%\")\n",
    "\n",
    "    # Apply Canny edge detection to the cropped object\n",
    "    gray_cropped = cv2.cvtColor(cropped_object, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray_cropped, threshold1=150, threshold2=50, apertureSize=3, L2gradient=True)\n",
    "    # Display the cropped object with Canny edges\n",
    "    cropped_img = Image.fromarray(edges)\n",
    "    cropped_img = ImageTk.PhotoImage(image=cropped_img)\n",
    "    cropped_label.img = cropped_img  # To prevent garbage collection\n",
    "    cropped_label.config(image=cropped_img)\n",
    "\n",
    "    # Convert frame from OpenCV format to PIL format\n",
    "    img = cv2.cvtColor(processed_frame, cv2.COLOR_BGR2RGB)\n",
    "    img = Image.fromarray(img)\n",
    "    img = ImageTk.PhotoImage(image=img)\n",
    "\n",
    "    # Display the captured frame with processed frame and cropped object\n",
    "    frame_label.img = img  # To prevent garbage collection\n",
    "    frame_label.config(image=img)\n",
    "\n",
    "\n",
    "# Create GUI\n",
    "root = Tk()\n",
    "root.title(\"Defect Detection\")\n",
    "\n",
    "# Frame for the original frame and processed frame with bounding box\n",
    "frame_container = Frame(root)\n",
    "frame_container.pack(side=LEFT, padx=10, pady=10)\n",
    "\n",
    "# Label to display the captured frame with bounding box\n",
    "frame_label = Label(frame_container)\n",
    "frame_label.pack()\n",
    "\n",
    "# Label to display the cropped object with Canny edges\n",
    "cropped_label = Label(root)\n",
    "cropped_label.pack(side=RIGHT)\n",
    "\n",
    "# Label to display the processed image\n",
    "processed_label = Label(root)\n",
    "processed_label.pack()\n",
    "\n",
    "# Button to capture image\n",
    "capture_button = Button(root, text=\"Capture Image\", command=capture_image)\n",
    "capture_button.pack(pady=10)\n",
    "\n",
    "# Label to display the prediction result\n",
    "result_label = Label(root, text=\"\")\n",
    "result_label.pack(pady=5)\n",
    "\n",
    "# Label to display the prediction confidence\n",
    "accuracy_label = Label(root, text=\"\")\n",
    "accuracy_label.pack(pady=5)\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CANNY SIMULATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Initialize the webcam capture\n",
    "cap = cv2.VideoCapture(1, cv2.CAP_DSHOW)  # 0 represents the default camera (you may need to change it)\n",
    "\n",
    "while True:\n",
    "    # Capture a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # Break the loop if no frame is read\n",
    "\n",
    "    # Convert the frame to grayscale (required for Canny)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply Canny edge detection\n",
    "    edges = cv2.Canny(gray, threshold1=150, threshold2=60, apertureSize=3)  # Adjust thresholds as needed\n",
    "\n",
    "    # Display the Canny edges\n",
    "    cv2.imshow(\"Canny Edge Detection\", edges)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam capture\n",
    "cap.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
